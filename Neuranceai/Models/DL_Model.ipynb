{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a265e85b",
   "metadata": {},
   "source": [
    "<h1> 1 Installing Libraries </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33c88e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import check_output\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from subprocess import check_output\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from os import path\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine # database connection\n",
    "import csv\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics.classification import accuracy_score, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "# from sklearn.cross_validation import StratifiedKFold \n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import math\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve\n",
    "\n",
    "\n",
    "# DLL\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf1\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import add\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import utils\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (Attention, \n",
    "                                     Layer,\n",
    "                                     Add, concatenate, \n",
    "                                     Input, \n",
    "                                     Dense,  \n",
    "                                     LSTM, Bidirectional, GRU,\n",
    "                                     ZeroPadding2D, \n",
    "                                     Convolution2D, Conv2D, \n",
    "                                     GlobalAveragePooling2D, GlobalAvgPool2D, GlobalMaxPooling2D, GlobalMaxPool2D, \n",
    "                                     AveragePooling2D, AvgPool2D, MaxPooling2D, MaxPool2D,\n",
    "                                     Flatten,\n",
    "                                     BatchNormalization, \n",
    "                                     Dropout)\n",
    "\n",
    "from tensorflow.keras.layers import (Activation, \n",
    "                                     ReLU, \n",
    "                                     LeakyReLU, \n",
    "                                     Softmax)\n",
    "\n",
    "from tensorflow.keras.optimizers import (SGD,\n",
    "                                         Adam,\n",
    "                                         Adagrad,\n",
    "                                         Adadelta,\n",
    "                                         RMSprop,\n",
    "                                         Nadam)\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img, load_img\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30fecd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt(name):\n",
    "    with open(name, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def load_pickle(name):\n",
    "    with open(name, \"rb\") as handle:\n",
    "        return pickle.load(handle)\n",
    "    \n",
    "def dump_pickle(name, file):\n",
    "    with open(name, \"wb\") as handle:\n",
    "        pickle.dump(file, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "889bbe5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>name_of_drug</th>\n",
       "      <th>use_case_for_drug</th>\n",
       "      <th>review_by_patient</th>\n",
       "      <th>effectiveness_rating</th>\n",
       "      <th>drug_approved_by_UIC</th>\n",
       "      <th>number_of_times_prescribed</th>\n",
       "      <th>base_score</th>\n",
       "      <th>review_len</th>\n",
       "      <th>review_n_words</th>\n",
       "      <th>review_by_patient_filter</th>\n",
       "      <th>review_by_patient_unique_word</th>\n",
       "      <th>len_review_by_patient_unique_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10091</th>\n",
       "      <td>223808</td>\n",
       "      <td>Depakote ER</td>\n",
       "      <td>Epilepsy</td>\n",
       "      <td>\"After trying 4 different meds this one was th...</td>\n",
       "      <td>8</td>\n",
       "      <td>1-Apr-08</td>\n",
       "      <td>15</td>\n",
       "      <td>7.633384</td>\n",
       "      <td>370</td>\n",
       "      <td>74</td>\n",
       "      <td>after trying 4 different meds this one was the...</td>\n",
       "      <td>['one', 'gladly', 'stayed', 'scary', 'well', '...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>35602</td>\n",
       "      <td>Anexsia</td>\n",
       "      <td>Pain</td>\n",
       "      <td>\"Worked better than Tylenol w/codeine #3 and C...</td>\n",
       "      <td>10</td>\n",
       "      <td>1-Apr-08</td>\n",
       "      <td>19</td>\n",
       "      <td>6.127475</td>\n",
       "      <td>88</td>\n",
       "      <td>14</td>\n",
       "      <td>worked better than tylenol w codeine  3 and ca...</td>\n",
       "      <td>['w', 'tylenol', '3', 'better', 'pain', 'caris...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       patient_id name_of_drug use_case_for_drug  \\\n",
       "10091      223808  Depakote ER          Epilepsy   \n",
       "1009        35602      Anexsia              Pain   \n",
       "\n",
       "                                       review_by_patient  \\\n",
       "10091  \"After trying 4 different meds this one was th...   \n",
       "1009   \"Worked better than Tylenol w/codeine #3 and C...   \n",
       "\n",
       "       effectiveness_rating drug_approved_by_UIC  number_of_times_prescribed  \\\n",
       "10091                     8             1-Apr-08                          15   \n",
       "1009                     10             1-Apr-08                          19   \n",
       "\n",
       "       base_score  review_len  review_n_words  \\\n",
       "10091    7.633384         370              74   \n",
       "1009     6.127475          88              14   \n",
       "\n",
       "                                review_by_patient_filter  \\\n",
       "10091  after trying 4 different meds this one was the...   \n",
       "1009   worked better than tylenol w codeine  3 and ca...   \n",
       "\n",
       "                           review_by_patient_unique_word  \\\n",
       "10091  ['one', 'gladly', 'stayed', 'scary', 'well', '...   \n",
       "1009   ['w', 'tylenol', '3', 'better', 'pain', 'caris...   \n",
       "\n",
       "       len_review_by_patient_unique_word  \n",
       "10091                                 35  \n",
       "1009                                  10  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_train_data = pd.read_csv(\"drug_preprocess.csv\", index_col=\"Unnamed: 0\")\n",
    "drug_train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bec21e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the results in 4 decemal points\n",
    "SAFE_DIV = 0.0001 \n",
    "\n",
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "stopword = set(STOP_WORDS)\n",
    "stopword.add(\"said\")\n",
    "stopword.add(\"br\")\n",
    "stopword.add(\" \")\n",
    "stopword.remove(\"not\")\n",
    "stopword.remove(\"no\")\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    x = str(x).lower()\n",
    "    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"asap\", \"as soon as possible\")\n",
    "    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
    "    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
    "    \n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    pattern = re.compile('\\W')\n",
    "    \n",
    "    if type(x) == type(''):\n",
    "        x = re.sub(pattern, ' ', x)\n",
    "    \n",
    "    \n",
    "    if type(x) == type(''):\n",
    "        x = porter.stem(x)\n",
    "        example1 = BeautifulSoup(x)\n",
    "        x = example1.get_text()\n",
    "               \n",
    "    return x\n",
    "def combine_name_use_review(a, b, c):\n",
    "    return preprocess(\" \".join([a, b, c])) \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e2088b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>name_of_drug</th>\n",
       "      <th>use_case_for_drug</th>\n",
       "      <th>review_by_patient</th>\n",
       "      <th>effectiveness_rating</th>\n",
       "      <th>drug_approved_by_UIC</th>\n",
       "      <th>number_of_times_prescribed</th>\n",
       "      <th>base_score</th>\n",
       "      <th>review_len</th>\n",
       "      <th>review_n_words</th>\n",
       "      <th>review_by_patient_filter</th>\n",
       "      <th>review_by_patient_unique_word</th>\n",
       "      <th>len_review_by_patient_unique_word</th>\n",
       "      <th>combine_name_use_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10091</th>\n",
       "      <td>223808</td>\n",
       "      <td>Depakote ER</td>\n",
       "      <td>Epilepsy</td>\n",
       "      <td>\"After trying 4 different meds this one was th...</td>\n",
       "      <td>8</td>\n",
       "      <td>1-Apr-08</td>\n",
       "      <td>15</td>\n",
       "      <td>7.633384</td>\n",
       "      <td>370</td>\n",
       "      <td>74</td>\n",
       "      <td>after trying 4 different meds this one was the...</td>\n",
       "      <td>['one', 'gladly', 'stayed', 'scary', 'well', '...</td>\n",
       "      <td>35</td>\n",
       "      <td>depakote er epilepsy after trying 4 different ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>35602</td>\n",
       "      <td>Anexsia</td>\n",
       "      <td>Pain</td>\n",
       "      <td>\"Worked better than Tylenol w/codeine #3 and C...</td>\n",
       "      <td>10</td>\n",
       "      <td>1-Apr-08</td>\n",
       "      <td>19</td>\n",
       "      <td>6.127475</td>\n",
       "      <td>88</td>\n",
       "      <td>14</td>\n",
       "      <td>worked better than tylenol w codeine  3 and ca...</td>\n",
       "      <td>['w', 'tylenol', '3', 'better', 'pain', 'caris...</td>\n",
       "      <td>10</td>\n",
       "      <td>anexsia pain worked better than tylenol w code...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       patient_id name_of_drug use_case_for_drug  \\\n",
       "10091      223808  Depakote ER          Epilepsy   \n",
       "1009        35602      Anexsia              Pain   \n",
       "\n",
       "                                       review_by_patient  \\\n",
       "10091  \"After trying 4 different meds this one was th...   \n",
       "1009   \"Worked better than Tylenol w/codeine #3 and C...   \n",
       "\n",
       "       effectiveness_rating drug_approved_by_UIC  number_of_times_prescribed  \\\n",
       "10091                     8             1-Apr-08                          15   \n",
       "1009                     10             1-Apr-08                          19   \n",
       "\n",
       "       base_score  review_len  review_n_words  \\\n",
       "10091    7.633384         370              74   \n",
       "1009     6.127475          88              14   \n",
       "\n",
       "                                review_by_patient_filter  \\\n",
       "10091  after trying 4 different meds this one was the...   \n",
       "1009   worked better than tylenol w codeine  3 and ca...   \n",
       "\n",
       "                           review_by_patient_unique_word  \\\n",
       "10091  ['one', 'gladly', 'stayed', 'scary', 'well', '...   \n",
       "1009   ['w', 'tylenol', '3', 'better', 'pain', 'caris...   \n",
       "\n",
       "       len_review_by_patient_unique_word  \\\n",
       "10091                                 35   \n",
       "1009                                  10   \n",
       "\n",
       "                                 combine_name_use_review  \n",
       "10091  depakote er epilepsy after trying 4 different ...  \n",
       "1009   anexsia pain worked better than tylenol w code...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_train_data[\"combine_name_use_review\"] = drug_train_data.loc[:, [\"name_of_drug\", \"use_case_for_drug\", \"review_by_patient_filter\"]].fillna(\"\").apply(lambda x: combine_name_use_review(x[\"name_of_drug\"], x[\"use_case_for_drug\"], x[\"review_by_patient_filter\"]), axis=1)\n",
    "drug_train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "df200932",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_name_use_review = list(drug_train_data[\"combine_name_use_review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "99f35a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary contains 28094 words\n"
     ]
    }
   ],
   "source": [
    "def create_vocabulary():       # no use\n",
    "    vocabulary = set()\n",
    "    [vocabulary.update(sent.split()) for sent in drug_train_data[\"combine_name_use_review\"].values]\n",
    "    return vocabulary\n",
    "vocabulary = create_vocabulary()\n",
    "print(f\"vocabulary contains {len(vocabulary)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fc4fe0",
   "metadata": {},
   "source": [
    "# 2: Create vocab of most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9829cbbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "i\n",
      "a\n",
      "w\n",
      "3\n",
      "m\n",
      "2\n",
      "5\n",
      "p\n",
      "s\n",
      "t\n",
      "v\n",
      "6\n",
      "1\n",
      "9\n",
      "e\n",
      "g\n",
      "7\n",
      "b\n",
      "d\n",
      "0\n",
      "8\n",
      "l\n",
      "c\n",
      "u\n",
      "h\n",
      "x\n",
      "q\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "z\n",
      "y\n",
      "k\n",
      "j\n",
      "_\n",
      "і\n",
      "38\n",
      "Vocabulary = 3164 in our trainig data\n"
     ]
    }
   ],
   "source": [
    "def create_vocab(word_count_threshold = 10):\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in all_train_name_use_review:\n",
    "        nsents += 1\n",
    "        for w in sent.split(' '):\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "    c = 0\n",
    "    for key, val in word_counts.items():\n",
    "        if len(key) == 1:\n",
    "            c += 1\n",
    "            print(key)\n",
    "    print(c)\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "    return vocab, word_counts\n",
    "vocab, word_counts = create_vocab(50)\n",
    "print('Vocabulary = %d in our trainig data' % (len(vocab)))    # 10 -> 5607, 7-> 6705, 15-> 4477, 30->2985, 50 -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0e2bd560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n",
      "size of vocabulary = 3167 in our trainig data\n"
     ]
    }
   ],
   "source": [
    "def ixtoword_wordtoix():\n",
    "    ixtoword = {}\n",
    "    wordtoix = {}\n",
    "    ix = 1\n",
    "    for w in vocab:\n",
    "        wordtoix[w] = ix\n",
    "        ixtoword[ix] = w\n",
    "        ix += 1\n",
    "    return ixtoword, wordtoix\n",
    "if 'wordtoix.pkl' in os.listdir(\"../ix_word\"):\n",
    "    print(\"load\")\n",
    "    ixtoword = load_pickle('../ix_word/ixtoword.pkl')\n",
    "    wordtoix = load_pickle('../ix_word/wordtoix.pkl')\n",
    "else:\n",
    "    ixtoword, wordtoix = ixtoword_wordtoix()\n",
    "    dump_pickle('../ix_word/ixtoword.pkl', ixtoword)\n",
    "    dump_pickle('../ix_word/wordtoix.pkl', wordtoix)\n",
    "    print(\"dump\")\n",
    "vocab_size = len(ixtoword) + 1\n",
    "print('size of vocabulary = %d in our trainig data' % (vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "908e3598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3167"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc2642",
   "metadata": {},
   "source": [
    "# 3 Maximum length of caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c2906fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max(len(d.split()) for d in all_train_name_use_review)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c00083",
   "metadata": {},
   "source": [
    "<h1>4 Embadding index and matrix </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8070b90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n",
      "There are 400000 no of words in glove\n"
     ]
    }
   ],
   "source": [
    "def create_embadding_index():\n",
    "    embeddings_index = {} \n",
    "    f = open(os.path.join(glove_path, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "if \"embeddings_index.pickle\" in os.listdir(r\"D:\\computer vision\\image_caption\"):\n",
    "    print('load')\n",
    "    embeddings_index = load_pickle(os.path.join(r\"D:\\computer vision\\image_caption\", \"embeddings_index.pickle\"))\n",
    "else:\n",
    "    print(\"dump\")\n",
    "    embeddings_index = create_embadding_index()\n",
    "    dump_pickle(os.path.join(\"../ix_word\", \"embeddings_index.pkl\"), embeddings_index)\n",
    "\n",
    "print(f\"There are {len(embeddings_index)} no of words in glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e81e2552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n",
      "The shape of glove representation is (3167, 200)\n"
     ]
    }
   ],
   "source": [
    "def create_embadding_matrix():\n",
    "    embedding_dim = 200\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in wordtoix.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            print(i, word)\n",
    "    return embedding_matrix\n",
    "if \"embeddings_glove.pkl\" in os.listdir(\"../ix_word\"):\n",
    "    print('load')\n",
    "    embeddings_glove = load_pickle(os.path.join(\"../ix_word\", \"embeddings_glove.pkl\"))\n",
    "else:\n",
    "    print(\"dump\")\n",
    "    embeddings_glove = create_embadding_matrix()\n",
    "    dump_pickle(os.path.join(\"../ix_word\", \"embeddings_glove.pkl\"), embeddings_glove)\n",
    "\n",
    "print(f\"The shape of glove representation is {embeddings_glove.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ad8f0",
   "metadata": {},
   "source": [
    "# 5 DistilBert Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7ed26a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Refer: https://huggingface.co/transformers/model_doc/distilbert.html#\n",
    "\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "\n",
    "distil_bert = 'distilbert-base-uncased' # Name of the pretrained models\n",
    "\n",
    "#DistilBERT \n",
    "tokenizer = DistilBertTokenizer.from_pretrained(distil_bert)\n",
    "model1 = TFDistilBertModel.from_pretrained(distil_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d5c9193c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3166"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordtoix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b374e584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n",
      "The shape of distil representation is (3167, 768)\n"
     ]
    }
   ],
   "source": [
    "def find_vec(word):\n",
    "    vec = [tokenizer.encode(word)[1]]\n",
    "    vec = tf.constant(vec)[None, :]\n",
    "    return model1(vec)[0][0, 0, :].numpy()\n",
    "\n",
    "distill_bert_embedding = np.zeros((vocab_size, 768))\n",
    "def create_embadding_distil():\n",
    "    global distill_bert_embedding\n",
    "    embedding_dim = 768\n",
    "    for word, i in wordtoix.items():\n",
    "        embedding_vector = find_vec(word)\n",
    "        print(i)\n",
    "        if embedding_vector is not None:\n",
    "            distill_bert_embedding[i] = embedding_vector\n",
    "        else:\n",
    "            print(i, word)\n",
    "    return distill_bert_embedding\n",
    "\n",
    "if \"embeddings_distil.pkl\" in os.listdir(\"../ix_word/embedding\"):\n",
    "    print('load')\n",
    "    embeddings_distil = load_pickle(r\"../ix_word/embedding/embeddings_distil.pkl\")\n",
    "else:\n",
    "    print(\"dump\")\n",
    "    embeddings_distil = create_embadding_distil()\n",
    "    dump_pickle(r\"../ix_word/embedding/embeddings_distil.pkl\", embeddings_distil)\n",
    "\n",
    "print(f\"The shape of distil representation is {embeddings_distil.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "984e852d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3167"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "32fd3df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = drug_train_data.iloc[:30000, :]\n",
    "validate_data = drug_train_data.iloc[30000:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9671f8d9",
   "metadata": {},
   "source": [
    "<h1> Model Architecture </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a40535a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 650)]        0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 650, 768)     2432256     ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_42 (Dropout)           (None, 5)            0           ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)           (None, 650, 768)     0           ['embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8)            48          ['dropout_42[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 650, 768)    3072        ['dropout_41[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 8)           32          ['dense_17[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirectional  (None, 64)          205056      ['batch_normalization_22[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)                (None, 8)            0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 72)           0           ['bidirectional_5[0][0]',        \n",
      "                                                                  're_lu_12[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_43 (Dropout)           (None, 72)           0           ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 72)          288         ['dropout_43[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 5)            365         ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)                (None, 5)            0           ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_44 (Dropout)           (None, 5)            0           ['re_lu_13[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 5)           20          ['dropout_44[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 1)            6           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,641,143\n",
      "Trainable params: 2,639,437\n",
      "Non-trainable params: 1,706\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " [<keras.engine.input_layer.InputLayer at 0x1c9b6abb0d0>,\n",
       "  <keras.engine.input_layer.InputLayer at 0x1c9809b01c0>,\n",
       "  <keras.layers.embeddings.Embedding at 0x1c9b6abb190>,\n",
       "  <keras.layers.core.dropout.Dropout at 0x1c9b207b400>,\n",
       "  <keras.layers.core.dropout.Dropout at 0x1c9b6abb310>,\n",
       "  <keras.layers.core.dense.Dense at 0x1c9c553bee0>,\n",
       "  <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1c9b6abb130>,\n",
       "  <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1c9c86a5eb0>,\n",
       "  <keras.layers.wrappers.Bidirectional at 0x1c9b207b790>,\n",
       "  <keras.layers.advanced_activations.ReLU at 0x1c9809a1910>,\n",
       "  <keras.layers.merge.Concatenate at 0x1c9b6f10fa0>,\n",
       "  <keras.layers.core.dropout.Dropout at 0x1c9809b0b20>,\n",
       "  <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1c9c8ab5a30>,\n",
       "  <keras.layers.core.dense.Dense at 0x1c9c9de02e0>,\n",
       "  <keras.layers.advanced_activations.ReLU at 0x1c9b2499160>,\n",
       "  <keras.layers.core.dropout.Dropout at 0x1c9c9e15f40>,\n",
       "  <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1c9c1f81d60>,\n",
       "  <keras.layers.core.dense.Dense at 0x1c9bf4a03a0>])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim =768\n",
    "vocab_size = 3167\n",
    "max_length = 650\n",
    "lstm_input_shape = 768\n",
    "mlp_input_shape = 5\n",
    "\n",
    "# RNN - input --> bert embedding --> LSTM --> LSTM\n",
    "inputs1 = Input(shape=(max_length,)) # 768\n",
    "em1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs1)\n",
    "se1 = Dropout(0.2)(em1)\n",
    "se1 = BatchNormalization()(se1)\n",
    "se1 = Bidirectional(LSTM(32))(se1)\n",
    "# se2 = Dropout(0.2)(em1)\n",
    "# se2 = BatchNormalization()(se2)\n",
    "# se2 = Bidirectional(LSTM(64))(se2)\n",
    "\n",
    "inputs2 = Input(shape=(mlp_input_shape,)) # 5\n",
    "fe1 = Dropout(0.2)(inputs2)\n",
    "fe1 = Dense(8)(fe1)\n",
    "fe1 = BatchNormalization()(fe1)\n",
    "fe1 = ReLU()(fe1)\n",
    "\n",
    "\n",
    "# Add both LSTM and CNN\n",
    "decoder1 = concatenate([se1, fe1])\n",
    "decoder1 = Dropout(0.2)(decoder1)\n",
    "decoder1 = BatchNormalization()(decoder1)\n",
    "decoder1 = Dense(5)(decoder1)\n",
    "decoder1 = ReLU()(decoder1)\n",
    "\n",
    "decoder1 = Dropout(0.2)(decoder1)\n",
    "decoder1 = BatchNormalization()(decoder1)\n",
    "outputs = Dense(1, kernel_initializer='normal', activation='linear')(decoder1)\n",
    "\n",
    "m1 = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "m1.summary(), m1.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f35b7591",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.layers[2].set_weights([embeddings_distil])\n",
    "m1.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ebedab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>name_of_drug</th>\n",
       "      <th>use_case_for_drug</th>\n",
       "      <th>review_by_patient</th>\n",
       "      <th>effectiveness_rating</th>\n",
       "      <th>drug_approved_by_UIC</th>\n",
       "      <th>number_of_times_prescribed</th>\n",
       "      <th>base_score</th>\n",
       "      <th>review_len</th>\n",
       "      <th>review_n_words</th>\n",
       "      <th>review_by_patient_filter</th>\n",
       "      <th>review_by_patient_unique_word</th>\n",
       "      <th>len_review_by_patient_unique_word</th>\n",
       "      <th>combine_name_use_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10091</th>\n",
       "      <td>223808</td>\n",
       "      <td>Depakote ER</td>\n",
       "      <td>Epilepsy</td>\n",
       "      <td>\"After trying 4 different meds this one was th...</td>\n",
       "      <td>8</td>\n",
       "      <td>1-Apr-08</td>\n",
       "      <td>15</td>\n",
       "      <td>7.633384</td>\n",
       "      <td>370</td>\n",
       "      <td>74</td>\n",
       "      <td>after trying 4 different meds this one was the...</td>\n",
       "      <td>['effect', 'trying', '4', 'possible', 'quit', ...</td>\n",
       "      <td>35</td>\n",
       "      <td>depakote er epilepsy after trying 4 different ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>35602</td>\n",
       "      <td>Anexsia</td>\n",
       "      <td>Pain</td>\n",
       "      <td>\"Worked better than Tylenol w/codeine #3 and C...</td>\n",
       "      <td>10</td>\n",
       "      <td>1-Apr-08</td>\n",
       "      <td>19</td>\n",
       "      <td>6.127475</td>\n",
       "      <td>88</td>\n",
       "      <td>14</td>\n",
       "      <td>worked better than tylenol w codeine  3 and ca...</td>\n",
       "      <td>['3', 'carisoprodol', 'better', 'codeine', 'ty...</td>\n",
       "      <td>10</td>\n",
       "      <td>anexsia pain worked better than tylenol w code...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       patient_id name_of_drug use_case_for_drug  \\\n",
       "10091      223808  Depakote ER          Epilepsy   \n",
       "1009        35602      Anexsia              Pain   \n",
       "\n",
       "                                       review_by_patient  \\\n",
       "10091  \"After trying 4 different meds this one was th...   \n",
       "1009   \"Worked better than Tylenol w/codeine #3 and C...   \n",
       "\n",
       "       effectiveness_rating drug_approved_by_UIC  number_of_times_prescribed  \\\n",
       "10091                     8             1-Apr-08                          15   \n",
       "1009                     10             1-Apr-08                          19   \n",
       "\n",
       "       base_score  review_len  review_n_words  \\\n",
       "10091    7.633384         370              74   \n",
       "1009     6.127475          88              14   \n",
       "\n",
       "                                review_by_patient_filter  \\\n",
       "10091  after trying 4 different meds this one was the...   \n",
       "1009   worked better than tylenol w codeine  3 and ca...   \n",
       "\n",
       "                           review_by_patient_unique_word  \\\n",
       "10091  ['effect', 'trying', '4', 'possible', 'quit', ...   \n",
       "1009   ['3', 'carisoprodol', 'better', 'codeine', 'ty...   \n",
       "\n",
       "       len_review_by_patient_unique_word  \\\n",
       "10091                                 35   \n",
       "1009                                  10   \n",
       "\n",
       "                                 combine_name_use_review  \n",
       "10091  depakote er epilepsy after trying 4 different ...  \n",
       "1009   anexsia pain worked better than tylenol w code...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1763d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "#     tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "    tf.keras.metrics.MeanAbsoluteError(),\n",
    "    tf.keras.metrics.MeanSquaredError()\n",
    "]\n",
    "# m1.compile(loss='mean_squared_error', optimizer='adam')\n",
    "m1.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.0001), metrics=[\"accuracy\", METRICS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "61088561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(train_data, max_length, num_record_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    while 1:\n",
    "        for i in range(drug_train_data.shape[0]):\n",
    "            n+=1\n",
    "            # retrieve the text feature\n",
    "            text = drug_train_data.iloc[0, 10]\n",
    "            seq = [wordtoix[word] for word in text.split(' ') if word in wordtoix]\n",
    "            seq = pad_sequences([seq], maxlen=max_length)[0]\n",
    "            X1.append(seq)\n",
    "            \n",
    "            numerical_feature = list(drug_train_data.iloc[0, :][[\"effectiveness_rating\", \"number_of_times_prescribed\", \"review_len\", \"review_n_words\", \"len_review_by_patient_unique_word\", \"base_score\"]])\n",
    "            X2.append(numerical_feature[:-1])\n",
    "            y.append(numerical_feature[-1])\n",
    "\n",
    "            if n==num_record_per_batch:\n",
    "#                 print([array(X1).shape, array(X2).shape], array(y).shape)\n",
    "                yield ([array(X1), array(X2)], array(y))\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec1e14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = drug_train_data.iloc[:30000, :]\n",
    "validate_data = drug_train_data.iloc[30000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "85648204",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no 1\n",
      "1875/1875 [==============================] - 2237s 1s/step - loss: 52.9178 - accuracy: 0.0000e+00 - mean_absolute_error: 7.2592 - mean_squared_error: 52.9178\n",
      "epoch no 2\n",
      "1875/1875 [==============================] - 2340s 1s/step - loss: 33.4407 - accuracy: 0.0000e+00 - mean_absolute_error: 5.7196 - mean_squared_error: 33.4407\n",
      "epoch no 3\n",
      "1875/1875 [==============================] - 2303s 1s/step - loss: 13.5908 - accuracy: 0.0000e+00 - mean_absolute_error: 3.5545 - mean_squared_error: 13.5908\n",
      "epoch no 4\n",
      "1875/1875 [==============================] - 2285s 1s/step - loss: 2.7467 - accuracy: 0.0000e+00 - mean_absolute_error: 1.4481 - mean_squared_error: 2.7467\n",
      "epoch no 5\n",
      "1875/1875 [==============================] - 2348s 1s/step - loss: 0.3763 - accuracy: 0.0000e+00 - mean_absolute_error: 0.4697 - mean_squared_error: 0.3763\n",
      "epoch no 6\n",
      "1875/1875 [==============================] - 2310s 1s/step - loss: 0.1625 - accuracy: 0.0000e+00 - mean_absolute_error: 0.2982 - mean_squared_error: 0.1625\n",
      "epoch no 7\n",
      "1875/1875 [==============================] - 2322s 1s/step - loss: 0.0720 - accuracy: 0.0000e+00 - mean_absolute_error: 0.1917 - mean_squared_error: 0.0720\n",
      "epoch no 8\n",
      "1875/1875 [==============================] - 2435s 1s/step - loss: 0.0235 - accuracy: 0.0000e+00 - mean_absolute_error: 0.1030 - mean_squared_error: 0.0235\n",
      "epoch no 9\n",
      "  16/1875 [..............................] - ETA: 43:02 - loss: 0.0100 - accuracy: 0.0000e+00 - mean_absolute_error: 0.0651 - mean_squared_error: 0.0100"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nOOM when allocating tensor with shape[650,16,768] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node gradients/ReverseV2_grad/ReverseV2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[Adam/gradients/PartitionedCall_1]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_94553]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2120/29473441.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"epoch no {i}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mm1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m#     rfr_predicted_y = scaler_y.inverse_transform(rfr.predict(scaler_X.transform(validate_data_X[0:100])))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#     m1_predicted_y = m1.predict(validate_data_X[0:100]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nOOM when allocating tensor with shape[650,16,768] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node gradients/ReverseV2_grad/ReverseV2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[Adam/gradients/PartitionedCall_1]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_94553]"
     ]
    }
   ],
   "source": [
    "#tf.config.experimental_run_functions_eagerly(True)\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "steps = train_data.shape[0]//batch_size\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    print(f\"epoch no {i}\")\n",
    "    generator = data_generator(train_data, max_length, batch_size)\n",
    "    m1.fit(generator, epochs=1, batch_size=batch_size, steps_per_epoch=steps, verbose=1)\n",
    "#     rfr_predicted_y = scaler_y.inverse_transform(rfr.predict(scaler_X.transform(validate_data_X[0:100])))\n",
    "#     m1_predicted_y = m1.predict(validate_data_X[0:100]))\n",
    "#     print(\"mean sq\", mean_squared_error(validate_data_y[0:100], rfr_predicted_y))\n",
    "#     print(\"mean_absolute_error \",mean_absolute_error(validate_data_y[0:100], rfr_predicted_y))\n",
    "#     print(\"r2 Score\",r2_score(validate_data_y[0:100], rfr_predicted_y))\n",
    "    m1.save(f\"model/main_{i}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce9d8c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = keras.models.load_model(\"model/main_8.h5\")\n",
    "m1.optimizer.learning_rate = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445da8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no 9\n",
      "1875/1875 [==============================] - 4335s 2s/step - loss: 0.0100 - accuracy: 0.0000e+00 - mean_absolute_error: 0.0639 - mean_squared_error: 0.0100\n",
      "epoch no 10\n",
      "1875/1875 [==============================] - 3275s 2s/step - loss: 0.0080 - accuracy: 0.0000e+00 - mean_absolute_error: 0.0556 - mean_squared_error: 0.0080\n",
      "epoch no 11\n",
      "  49/1875 [..............................] - ETA: 54:27 - loss: 0.0074 - accuracy: 0.0000e+00 - mean_absolute_error: 0.0532 - mean_squared_error: 0.0074"
     ]
    }
   ],
   "source": [
    "#tf.config.experimental_run_functions_eagerly(True)\n",
    "epochs = 14\n",
    "batch_size = 16\n",
    "steps = train_data.shape[0]//batch_size\n",
    "\n",
    "for i in range(9, epochs+1):\n",
    "    print(f\"epoch no {i}\")\n",
    "    generator = data_generator(train_data, max_length, batch_size)\n",
    "    m1.fit(generator, epochs=1, batch_size=batch_size, steps_per_epoch=steps, verbose=1)\n",
    "#     rfr_predicted_y = scaler_y.inverse_transform(rfr.predict(scaler_X.transform(validate_data_X[0:100])))\n",
    "#     m1_predicted_y = m1.predict(validate_data_X[0:100]))\n",
    "#     print(\"mean sq\", mean_squared_error(validate_data_y[0:100], rfr_predicted_y))\n",
    "#     print(\"mean_absolute_error \",mean_absolute_error(validate_data_y[0:100], rfr_predicted_y))\n",
    "#     print(\"r2 Score\",r2_score(validate_data_y[0:100], rfr_predicted_y))\n",
    "    m1.save(f\"model/main_{i}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02f556",
   "metadata": {},
   "source": [
    "<h1>Result</h1>\n",
    "<h3>mean_absolute_error : 0.0556</h3>\n",
    "<h3>mean_squared_error  : 0.0080</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efcb283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ceb57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c937f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bbe01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f9d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c15418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0474562",
   "metadata": {},
   "source": [
    "<h1>Model Architecture</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9d6d051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, 650)]        0           []                               \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 650)         2600        ['input_23[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 5)           20          ['input_24[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " embedding_11 (Embedding)       (None, 650, 768)     2432256     ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_85 (Dropout)           (None, 5)            0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_84 (Dropout)           (None, 650, 768)     0           ['embedding_11[0][0]']           \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 8)            48          ['dropout_85[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 650, 768)    3072        ['dropout_84[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 8)           32          ['dense_35[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " bidirectional_11 (Bidirectiona  (None, 120)         397920      ['batch_normalization_57[0][0]'] \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " re_lu_24 (ReLU)                (None, 8)            0           ['batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 128)          0           ['bidirectional_11[0][0]',       \n",
      "                                                                  're_lu_24[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_86 (Dropout)           (None, 128)          0           ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 128)         512         ['dropout_86[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 5)            645         ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_25 (ReLU)                (None, 5)            0           ['dense_36[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_87 (Dropout)           (None, 5)            0           ['re_lu_25[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 5)           20          ['dropout_87[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_37 (Dense)               (None, 1)            6           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,837,131\n",
      "Trainable params: 2,834,003\n",
      "Non-trainable params: 3,128\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " [<keras.engine.input_layer.InputLayer at 0x1c981dd2a00>,\n",
       "  <keras.engine.input_layer.InputLayer at 0x1c9bb80ed60>,\n",
       "  <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1c981dd29a0>,\n",
       "  <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1c9bb81bb20>,\n",
       "  <keras.layers.embeddings.Embedding at 0x1c981e0cf10>,\n",
       "  <keras.layers.core.dropout.Dropout at 0x1c9bb809eb0>,\n",
       "  <keras.layers.core.dropout.Dropout at 0x1c981e08850>,\n",
       "  <keras.layers.core.dense.Dense at 0x1c9bb813430>,\n",
       "  <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1c981dd2790>,\n",
       "  <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1c9b6c10220>,\n",
       "  <keras.layers.wrappers.Bidirectional at 0x1c9bb822310>,\n",
       "  <keras.layers.advanced_activations.ReLU at 0x1c98da8abe0>,\n",
       "  <keras.layers.merge.Concatenate at 0x1c98155a550>,\n",
       "  <keras.layers.core.dropout.Dropout at 0x1c98155adc0>,\n",
       "  <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1c98daf8610>,\n",
       "  <keras.layers.core.dense.Dense at 0x1c9b1ccbd60>,\n",
       "  <keras.layers.advanced_activations.ReLU at 0x1c9b1ccbd00>,\n",
       "  <keras.layers.core.dropout.Dropout at 0x1c9c838c250>,\n",
       "  <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1c9c838cfd0>,\n",
       "  <keras.layers.core.dense.Dense at 0x1ca16742a00>])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim =768\n",
    "vocab_size = 3167\n",
    "max_length = 650\n",
    "lstm_input_shape = 768\n",
    "mlp_input_shape = 5\n",
    "\n",
    "# RNN - input --> bert embedding --> LSTM --> LSTM\n",
    "inputs1 = Input(shape=(max_length,)) # 768\n",
    "se1 = BatchNormalization()(inputs1)\n",
    "em1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(se1)\n",
    "se1 = Dropout(0.3)(em1)\n",
    "se1 = BatchNormalization()(se1)\n",
    "se1 = Bidirectional(LSTM(60))(se1)\n",
    "# se2 = Dropout(0.2)(em1)\n",
    "# se2 = BatchNormalization()(se2)\n",
    "# se2 = Bidirectional(LSTM(64))(se2)\n",
    "\n",
    "inputs2 = Input(shape=(mlp_input_shape,)) # 5\n",
    "fe1 = BatchNormalization()(inputs2)\n",
    "fe1 = Dropout(0.2)(fe1)\n",
    "fe1 = Dense(8)(fe1)\n",
    "fe1 = BatchNormalization()(fe1)\n",
    "fe1 = ReLU()(fe1)\n",
    "\n",
    "\n",
    "# Add both LSTM and CNN\n",
    "decoder1 = concatenate([se1, fe1])\n",
    "decoder1 = Dropout(0.3)(decoder1)\n",
    "decoder1 = BatchNormalization()(decoder1)\n",
    "decoder1 = Dense(5)(decoder1)\n",
    "decoder1 = ReLU()(decoder1)\n",
    "\n",
    "decoder1 = Dropout(0.2)(decoder1)\n",
    "decoder1 = BatchNormalization()(decoder1)\n",
    "outputs = Dense(1, kernel_initializer='normal', activation='linear')(decoder1)\n",
    "\n",
    "m1 = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "m1.summary(), m1.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "64ded833",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.layers[4].set_weights([embeddings_distil])\n",
    "m1.layers[4].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d2b89000",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    tf.keras.metrics.MeanAbsoluteError(),\n",
    "]\n",
    "# m1.compile(loss='mean_squared_error', optimizer='adam')\n",
    "m1.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.0001), metrics=[METRICS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2e350ee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no 1\n",
      "  37/1875 [..............................] - ETA: 1:23:26 - loss: 48.8417 - mean_absolute_error: 6.9887"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nOOM when allocating tensor with shape[650,16,768] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node gradients/TensorArrayUnstack/TensorListFromTensor_grad/TensorListStack}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[Adam/gradients/PartitionedCall_1]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_337686]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2120/295422613.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"epoch no {i}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mm1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mm1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"model/main2_{i}.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nOOM when allocating tensor with shape[650,16,768] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node gradients/TensorArrayUnstack/TensorListFromTensor_grad/TensorListStack}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[Adam/gradients/PartitionedCall_1]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_337686]"
     ]
    }
   ],
   "source": [
    "#tf.config.experimental_run_functions_eagerly(True)\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "steps = train_data.shape[0]//batch_size\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    print(f\"epoch no {i}\")\n",
    "    generator = data_generator(train_data, max_length, batch_size)\n",
    "    m1.fit(generator, epochs=1, batch_size=batch_size, steps_per_epoch=steps, verbose=1)\n",
    "    m1.save(f\"model/main2_{i}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2753b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb4f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
